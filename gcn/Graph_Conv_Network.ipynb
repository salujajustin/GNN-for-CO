{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph-Conv-Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2psz4OYutIDl"
      },
      "source": [
        "# Install required packages, set backend\n",
        "!pip install -q dgl         # For CPU Build\n",
        "!pip install -q dgl-cu101   # For CUDA 10.1 Build"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyWxazG2tYi7",
        "outputId": "6837f937-b14e-474b-9748-b5aaee823b83"
      },
      "source": [
        "# Import dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "import pdb\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "import dgl\n",
        "from dgl.nn.pytorch import GATConv\n",
        "from dgl.nn.pytorch.conv import gatedgraphconv\n",
        "!pip install -q dgl         # For CPU Build\n",
        "!pip install -q dgl-cu101   # For CUDA 10.1 Build\n",
        "from dgl.data import DGLDataset\n",
        "import os\n",
        "import numpy as np\n",
        "print(dgl.__version__)\n",
        "print(torch.__version__)\n",
        "print(np.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6.1\n",
            "1.8.1+cu101\n",
            "1.19.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3S5kd7I7YMF",
        "outputId": "f3f95aaf-86df-4c55-c502-e6030d858b45"
      },
      "source": [
        "!pip install gdown\n",
        "\n",
        "!mkdir -p data/tsp\n",
        "!cd data/tsp\n",
        "\n",
        "# Download tsp datasets (22mb each)\n",
        "!gdown --id 1tlcHok1JhOtQZOIshoGtyM5P9dZfnYbZ # tsp100_validation_seed4321.pkl\n",
        "!gdown --id 1woyNI8CoDJ8hyFko4NBJ6HdF4UQA0S77 # tsp100_test_seed1234.pkl\n",
        "\n",
        "!cd ../..\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tlcHok1JhOtQZOIshoGtyM5P9dZfnYbZ\n",
            "To: /content/tsp100_validation_seed4321.pkl\n",
            "22.0MB [00:00, 102MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1woyNI8CoDJ8hyFko4NBJ6HdF4UQA0S77\n",
            "To: /content/tsp100_test_seed1234.pkl\n",
            "22.0MB [00:00, 83.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL3aLa0f78hL",
        "outputId": "6687c34d-7c2c-4905-e60e-83a73b962b50"
      },
      "source": [
        "!pip install pickle5\n",
        "import os\n",
        "import pickle5 as pickle\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def check_extension(filename):\n",
        "    if os.path.splitext(filename)[1] != \".pkl\":\n",
        "        return filename + \".pkl\"\n",
        "    return filename\n",
        "\n",
        "def load_dataset(filename):\n",
        "\n",
        "    with open(check_extension(filename), 'rb') as f:\n",
        "      return pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pickle5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/5c4dd0462c8d3a6bc4af500a6af240763c2ebd1efdc736fc2c946d44b70a/pickle5-0.0.11.tar.gz (132kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 61kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102kB 3.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 112kB 3.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 122kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 3.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pickle5\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle5: filename=pickle5-0.0.11-cp37-cp37m-linux_x86_64.whl size=219244 sha256=fbb401f419c79b798781bf59698df3d1562fa38008877df42deeae8fe40b72ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/90/95/f889ca4aa8b0e0c7f21c8470b6f5d6032f0390a3a141a9a3bd\n",
            "Successfully built pickle5\n",
            "Installing collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Joy0z3e7pgO"
      },
      "source": [
        "a = load_dataset('/content/tsp100_test_seed1234')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsjMbpi9f_YA",
        "outputId": "d9c4f1bb-7ba6-44d3-9bf0-3d6b780c2ca1"
      },
      "source": [
        "print(np.array(a[0]).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDIQU69hMi3a",
        "outputId": "1f20268b-4ba8-43ef-ed9f-2904b1a84df6"
      },
      "source": [
        "import numpy as np\n",
        "def generate(TSP_nodes, seed_gen = 0 , distribution=\"uniform\", mode = \n",
        "np.float, map_size=(100, 100)):\n",
        "  ''' generates nodes in 2D space following the specified random distribution\n",
        "    Inputs:\n",
        "      TSP_nodes - number of nodes (int)\n",
        "      seed_gen - numpy seed (int)\n",
        "      distribution - np random distribution (string)\n",
        "      mode - np datatype\n",
        "      map_size - 2D space (int tuple)\n",
        "    Output:\n",
        "      nodes - (nx2 array) 2D spatial distribution of n nodes\n",
        "  '''\n",
        "  np.random.seed(seed_gen)\n",
        "  min_coords = min(map_size)\n",
        "  if distribution == 'uniform':\n",
        "    tsp_coords = np.random.uniform(0,min_coords,size = (TSP_nodes,2))\n",
        "    tsp_coords = np.array(tsp_coords,dtype = mode)        \n",
        "  if distribution == 'normal':\n",
        "    mu = min_coords//2\n",
        "    sigma = int(min_coords*0.2)\n",
        "    tsp_coords = np.random.normal(mu,sigma,size = (self.num_nodes,2))\n",
        "    tsp_coords = np.array(tsp_coords,dtype = mode)\n",
        "  nodes = abs(tsp_coords).astype(float)\n",
        "  return nodes\n",
        "\n",
        "def calculate_edges(nodes):\n",
        "  ''' generates edges between all unique pairs of nodes weighted by euclidean distance\n",
        "    Input:\n",
        "      nodes - (nx2 array) 2D spatial distribution of n nodes\n",
        "    Output:\n",
        "      src - (n-1)*n/2 length array of source nodes\n",
        "      dst - (n-1)*n/2 length array of destination nodes\n",
        "\n",
        "  '''\n",
        "  num_nodes = nodes.shape[0]\n",
        "  src, dst = list(), list()\n",
        "  weights = np.zeros((num_nodes,num_nodes))\n",
        "  for i in range(weights.shape[0]):\n",
        "    for j in range(weights.shape[0]):\n",
        "      weights[i,j] = np.linalg.norm(nodes[i]-nodes[j])\n",
        "  neighbors = np.ones((num_nodes,num_nodes))\n",
        "  np.fill_diagonal(neighbors, 2)\n",
        "  node_id = np.arange(num_nodes)\n",
        "  for i in range(len(nodes)-1):\n",
        "    for j in range(i+1, len(nodes)):\n",
        "      src.append(i)\n",
        "      dst.append(j)\n",
        "      eucl_dist = (np.linalg.norm(nodes[i]-nodes[j]))\n",
        "  return np.array(src), np.array(dst), weights, neighbors, node_id\n",
        "\n",
        "#nodes = generate(TSP_nodes = 50, seed_gen = 1, distribution='uniform', mode=np.int)\n",
        "src, dist, weights, adj, node_id = calculate_edges(nodes)\n",
        "print(nodes.shape)\n",
        "print(src.shape, dist.shape)\n",
        "print(weights.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 2)\n",
            "(1225,) (1225,)\n",
            "(50, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCpNmHhAMnL6"
      },
      "source": [
        "\n",
        "class EucTSPGraph(DGLDataset):\n",
        "    def __init__(self):\n",
        "        super().__init__(name='EucTSPGraph')\n",
        "\n",
        "    def process(self):\n",
        "        nodes_data = generate(TSP_nodes = 50, seed_gen = 1, distribution='uniform', mode=np.int)\n",
        "        edges_src, edges_dst, edges_weights,adj, node_id = calculate_edges(nodes_data)\n",
        "\n",
        "        nodes_data = torch.from_numpy(nodes_data.astype(np.float32)) \n",
        "        edges_src = torch.from_numpy(edges_src)\n",
        "        edges_dst = torch.from_numpy(edges_dst)\n",
        "        edges_weights = torch.from_numpy(edges_weights.astype(np.float32))\n",
        "        adjacency = torch.from_numpy(adj.astype(np.int))\n",
        "        node_id = torch.from_numpy(node_id)\n",
        "\n",
        "\n",
        "        self.graph = dgl.graph((edges_src, edges_dst), num_nodes=nodes_data.shape[0])\n",
        "        self.graph.ndata['node_coord'] = nodes_data\n",
        "        self.graph.ndata['weight'] = edges_weights\n",
        "        self.graph.ndata['adj'] =  adjacency\n",
        "        self.graph.ndata['label'] = node_id#torch.arange(start=0, end=nodes_data.shape[0])\n",
        "\n",
        "        # source: https://docs.dgl.ai/tutorials/blitz/6_load_data.html#sphx-glr-tutorials-blitz-6-load-data-py\n",
        "\n",
        "        # If your dataset is a node classification dataset, you will need to assign\n",
        "        # masks indicating whether a node belongs to training, validation, and test set.\n",
        "        n_nodes = nodes_data.shape[0]\n",
        "        n_train = int(n_nodes * 0.6)\n",
        "        n_val = int(n_nodes * 0.2)\n",
        "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
        "        train_mask[:n_train] = True\n",
        "        val_mask[n_train:n_train + n_val] = True\n",
        "        test_mask[n_train + n_val:] = True\n",
        "        self.graph.ndata['train_mask'] = train_mask\n",
        "        self.graph.ndata['val_mask'] = val_mask\n",
        "        self.graph.ndata['test_mask'] = test_mask\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.graph\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cKpng6wDd7w"
      },
      "source": [
        "class BatchNormNode(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(BatchNormNode, self).__init__()\n",
        "        self.batch_norm = nn.BatchNorm1d(hidden_dim, track_running_stats=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_trans = x.transpose(1, 2).contiguous()  # Reshape input: (batch_size, hidden_dim, num_nodes)\n",
        "        x_trans_bn = self.batch_norm(x_trans)\n",
        "        x_bn = x_trans_bn.transpose(1, 2).contiguous()  # Reshape to original shape\n",
        "        return x_bn\n",
        "class BatchNormEdge(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(BatchNormEdge, self).__init__()\n",
        "        self.batch_norm = nn.BatchNorm2d(hidden_dim, track_running_stats=False)\n",
        "\n",
        "    def forward(self, e):\n",
        "        e_trans = e.transpose(1, 3).contiguous()  # Reshape input: (batch_size, num_nodes, num_nodes, hidden_dim)\n",
        "        e_trans_bn = self.batch_norm(e_trans)\n",
        "        e_bn = e_trans_bn.transpose(1, 3).contiguous()  # Reshape to original\n",
        "        return e_bn\n",
        "class NodeFeatures(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(NodeFeatures, self).__init__()\n",
        "        self.U = nn.Linear(hidden_dim, hidden_dim, True)\n",
        "        self.V = nn.Linear(hidden_dim, hidden_dim, True)\n",
        "\n",
        "    def forward(self, x, edge_gate):\n",
        "        Ux = self.U(x)  # B x V x H\n",
        "        Vx = self.V(x)  # B x V x H\n",
        "        Vx = Vx.unsqueeze(1)  # extend Vx from \"B x V x H\" to \"B x 1 x V x H\"\n",
        "        gateVx = edge_gate * Vx  # B x V x V x H\n",
        "        x_new = Ux + torch.sum(gateVx, dim=2) / (1e-20 + torch.sum(edge_gate, dim=2))  # B x V x H\n",
        "        #sum reduction\n",
        "        #x_new = Ux + torch.sum(gateVx, dim=2)  # B x V x H\n",
        "        return x_new\n",
        "class EdgeFeatures(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(EdgeFeatures, self).__init__()\n",
        "        self.U = nn.Linear(hidden_dim, hidden_dim, True)\n",
        "        self.V = nn.Linear(hidden_dim, hidden_dim, True)\n",
        "        \n",
        "    def forward(self, x, e):\n",
        "        Ue = self.U(e)\n",
        "        Vx = self.V(x)\n",
        "        Wx = Vx.unsqueeze(1)  # Extend Vx from \"B x V x H\" to \"B x V x 1 x H\"\n",
        "        Vx = Vx.unsqueeze(2)  # extend Vx from \"B x V x H\" to \"B x 1 x V x H\"\n",
        "        e_new = Ue + Vx + Wx\n",
        "        return e_new\n",
        "class ResidualGatedGCNLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(ResidualGatedGCNLayer, self).__init__()\n",
        "        self.node_feat = NodeFeatures(hidden_dim)\n",
        "        self.edge_feat = EdgeFeatures(hidden_dim)\n",
        "        self.bn_node = BatchNormNode(hidden_dim)\n",
        "        self.bn_edge = BatchNormEdge(hidden_dim)\n",
        "\n",
        "    def forward(self, x, e):\n",
        "        e_in = e\n",
        "        x_in = x\n",
        "        # Edge convolution\n",
        "        e_tmp = self.edge_feat(x_in, e_in)  # B x V x V x H\n",
        "        # Compute edge gates\n",
        "        edge_gate = F.sigmoid(e_tmp)\n",
        "        # Node convolution\n",
        "        x_tmp = self.node_feat(x_in, edge_gate)\n",
        "        # Batch normalization\n",
        "        e_tmp = self.bn_edge(e_tmp)\n",
        "        x_tmp = self.bn_node(x_tmp)\n",
        "        # ReLU Activation\n",
        "        e = F.relu(e_tmp)\n",
        "        x = F.relu(x_tmp)\n",
        "        # Residual connection\n",
        "        x_new = x_in + x\n",
        "        e_new = e_in + e\n",
        "        return x_new, e_new\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim, L=2):\n",
        "        super(MLP, self).__init__()\n",
        "        self.L = L\n",
        "        U = []\n",
        "        for layer in range(self.L - 1):\n",
        "            U.append(nn.Linear(hidden_dim, hidden_dim, True))\n",
        "        self.U = nn.ModuleList(U)\n",
        "        self.V = nn.Linear(hidden_dim, output_dim, True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Ux = x\n",
        "        for U_i in self.U:\n",
        "            Ux = U_i(Ux)  # B x H\n",
        "            Ux = F.relu(Ux)  # B x H\n",
        "        y = self.V(Ux)  # B x O\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIDPRwxN09sU"
      },
      "source": [
        "def loss_edges(y_pred_edges, y_edges, edge_cw):\n",
        "    # Edge loss\n",
        "    y = F.log_softmax(y_pred_edges, dim=3)  # B x V x V x voc_edges\n",
        "    y = y.permute(0, 3, 1, 2)  # B x voc_edges x V x V\n",
        "    loss_edges = nn.NLLLoss(edge_cw)(y, y_edges)\n",
        "    return loss_edges\n",
        "def beamsearch_tour_nodes_shortest(y_pred_edges, x_edges_values, beam_size, batch_size, num_nodes,\n",
        "                                   dtypeFloat, dtypeLong, probs_type='raw', random_start=False):\n",
        "    \"\"\"\n",
        "    Performs beamsearch procedure on edge prediction matrices and returns possible TSP tours.\n",
        "    Final predicted tour is the one with the shortest tour length.\n",
        "    (Standard beamsearch returns the one with the highest probability and does not take length into account.)\n",
        "    Args:\n",
        "        y_pred_edges: Predictions for edges (batch_size, num_nodes, num_nodes)\n",
        "        x_edges_values: Input edge distance matrix (batch_size, num_nodes, num_nodes)\n",
        "        beam_size: Beam size\n",
        "        batch_size: Batch size\n",
        "        num_nodes: Number of nodes in TSP tours\n",
        "        dtypeFloat: Float data type (for GPU/CPU compatibility)\n",
        "        dtypeLong: Long data type (for GPU/CPU compatibility)\n",
        "        probs_type: Type of probability values being handled by beamsearch (either 'raw'/'logits'/'argmax'(TODO))\n",
        "        random_start: Flag for using fixed (at node 0) vs. random starting points for beamsearch\n",
        "    Returns:\n",
        "        shortest_tours: TSP tours in terms of node ordering (batch_size, num_nodes)\n",
        "    \"\"\"\n",
        "    if probs_type == 'raw':\n",
        "        # Compute softmax over edge prediction matrix\n",
        "        y = F.softmax(y_pred_edges, dim=3)  # B x V x V x voc_edges\n",
        "        # Consider the second dimension only\n",
        "        y = y[:, :, :, 1]  # B x V x V\n",
        "    elif probs_type == 'logits':\n",
        "        # Compute logits over edge prediction matrix\n",
        "        y = F.log_softmax(y_pred_edges, dim=3)  # B x V x V x voc_edges\n",
        "        # Consider the second dimension only\n",
        "        y = y[:, :, :, 1]  # B x V x V\n",
        "        y[y == 0] = -1e-20  # Set 0s (i.e. log(1)s) to very small negative number\n",
        "    # Perform beamsearch\n",
        "    beamsearch = Beamsearch(beam_size, batch_size, num_nodes, dtypeFloat, dtypeLong, probs_type, random_start)\n",
        "    trans_probs = y.gather(1, beamsearch.get_current_state())\n",
        "    for step in range(num_nodes - 1):\n",
        "        beamsearch.advance(trans_probs)\n",
        "        trans_probs = y.gather(1, beamsearch.get_current_state())\n",
        "    # Initially assign shortest_tours as most probable tours i.e. standard beamsearch\n",
        "    ends = torch.zeros(batch_size, 1).type(dtypeLong)\n",
        "    shortest_tours = beamsearch.get_hypothesis(ends)\n",
        "    # Compute current tour lengths\n",
        "    shortest_lens = [1e6] * len(shortest_tours)\n",
        "    for idx in range(len(shortest_tours)):\n",
        "        shortest_lens[idx] = tour_nodes_to_tour_len(shortest_tours[idx].cpu().numpy(),\n",
        "                                                    x_edges_values[idx].cpu().numpy())\n",
        "    # Iterate over all positions in beam (except position 0 --> highest probability)\n",
        "    for pos in range(1, beam_size):\n",
        "        ends = pos * torch.ones(batch_size, 1).type(dtypeLong)  # New positions\n",
        "        hyp_tours = beamsearch.get_hypothesis(ends)\n",
        "        for idx in range(len(hyp_tours)):\n",
        "            hyp_nodes = hyp_tours[idx].cpu().numpy()\n",
        "            hyp_len = tour_nodes_to_tour_len(hyp_nodes, x_edges_values[idx].cpu().numpy())\n",
        "            # Replace tour in shortest_tours if new length is shorter than current best\n",
        "            if hyp_len < shortest_lens[idx] and is_valid_tour(hyp_nodes, num_nodes):\n",
        "                shortest_tours[idx] = hyp_tours[idx]\n",
        "                shortest_lens[idx] = hyp_len\n",
        "    return shortest_tours"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-9gdEhpDv2T"
      },
      "source": [
        "class ResidualGatedGCNModel(nn.Module):\n",
        "    def __init__(self, num_nodes = 50, num_layers_graph = 30, num_layers_mpl = 3, hidden_size = 300):\n",
        "        super(ResidualGatedGCNModel, self).__init__()\n",
        "        # Define net parameters\n",
        "        self.num_nodes = num_nodes\n",
        "        self.node_dim = 2 #graph.node_dim\n",
        "        self.out_dim = 2 \n",
        "        self.hidden_dim = hidden_size\n",
        "        self.num_layers = num_layers_graph\n",
        "        self.mlp_layers = num_layers_mpl\n",
        "        # Node and edge embedding layers/lookups\n",
        "        self.nodes_coord_embedding = nn.Linear(self.node_dim, self.hidden_dim, bias=False)\n",
        "        self.edges_values_embedding = nn.Linear(1, self.hidden_dim//2, bias=False)\n",
        "        self.edges_embedding = nn.Embedding(self.node_dim + 1, self.hidden_dim//2)\n",
        "        # Define GCN Layers\n",
        "        gcn_layers = []\n",
        "        for layer in range(self.num_layers):\n",
        "            gcn_layers.append(ResidualGatedGCNLayer(self.hidden_dim))\n",
        "        self.gcn_layers = nn.ModuleList(gcn_layers)\n",
        "        # Define MLP classifiers\n",
        "        self.mlp_edges = MLP(self.hidden_dim, self.out_dim - 1, self.mlp_layers)\n",
        "        # self.mlp_nodes = MLP(self.hidden_dim, self.out_dim, self.mlp_layers)\n",
        "\n",
        "    def forward(self, x_edges, x_edges_values, x_nodes, x_nodes_coord):\n",
        "\n",
        "        # Node and edge embedding\n",
        "        x = self.nodes_coord_embedding(x_nodes_coord)  # B x V x H\n",
        "        e_vals = self.edges_values_embedding(x_edges_values.unsqueeze(3))  # B x V x V x H\n",
        "        e_tags = self.edges_embedding(x_edges)  # B x V x V x H\n",
        "        e = torch.cat((e_vals, e_tags), dim=3)\n",
        "        # GCN layers\n",
        "        for layer in range(self.num_layers):\n",
        "            x, e = self.gcn_layers[layer](x, e)  # B x V x H, B x V x V x H\n",
        "        # MLP classifier\n",
        "        y_pred_edges = self.mlp_edges(e)  \n",
        "        return y_pred_edges"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOiRB0Ha1wlv"
      },
      "source": [
        "def accuracy(logits, labels):\n",
        "    indices = torch.argmax(logits, dim=1)               # indices with highest value\n",
        "    num_correct = torch.sum(indices == labels)          # how many predictions match labels\n",
        "    return (num_correct.item()*1.0)/len(labels)         # convert to float and find percentage \n",
        "\n",
        "def evaluate(model, features, labels, mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():                               # deactivate autograd during eval\n",
        "        logits = model(features)\n",
        "        bs_nodes = beamsearch_tour_nodes_shortest(y_preds, x_edges_values, beam_size, batch_size, num_nodes, dtypeFloat, dtypeLong, probs_type='logits')\n",
        "        \n",
        "        # Compute mean tour length\n",
        "        pred_tour_len = mean_tour_len_nodes(x_edges_values, bs_nodes)\n",
        "        gt_tour_len = np.mean(batch.tour_len)\n",
        "\n",
        "        logits = logits[mask]\n",
        "        labels = labels[mask]\n",
        "        return accuracy(logits, labels)\n",
        "\n",
        "def train(model, features, labels, mask):\n",
        "    model.train()\n",
        "\"\"\"\n",
        "TO DO: incorporate labels generated by Concorde for edge mask generation\n",
        "\"\"\"\n",
        "    # Compute class weights (if uncomputed)\n",
        "    if type(edge_cw) != torch.Tensor:\n",
        "        edge_labels = y_edges.cpu().numpy().flatten()\n",
        "        edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
        "    \n",
        "    y_preds, loss = model(features[0].unsqueeze(0), features[1].unsqueeze(0), features[2].unsqueeze(0), features[3].unsqueeze(0))\n",
        "    # net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw)\n",
        "    #logp = F.log_softmax(logits, 1)\n",
        "    #loss = F.nll_loss(logp[mask], labels[mask])\n",
        "    loss = loss.mean()\n",
        "    \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "twr2IDgGM47S",
        "outputId": "4a018baa-2861-426a-da1c-69c4e34c28ea"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dataset = EucTSPGraph()\n",
        "# Dataset and attributes\n",
        "graph = dataset[0]                               # Only 1 graph in this dataset\n",
        "graph = graph.to(device)                      # Cast to GPU if available, else cpu\n",
        "node_features = graph.ndata['weight']                 # [2708, 1433]: each node has a word vector of 1433 unique words\n",
        "num_nodes = 50\n",
        "node_adj = graph.ndata['adj']\n",
        "node_coord = graph.ndata['node_coord']\n",
        "node_labels = graph.ndata['label']                  # [2708]: each node has one label of range [0-6]\n",
        "train_mask = graph.ndata['train_mask']\n",
        "valid_mask = graph.ndata['val_mask']\n",
        "test_mask = graph.ndata['test_mask']\n",
        "num_feats = node_features.size()[1]\n",
        "#num_classes = dataset.num_classes\n",
        "labels = node_labels\n",
        "features = [node_adj,node_features,node_labels,node_coord]\n",
        "# GAT Hyperparameters\n",
        "num_layers_graph = 30\n",
        "num_layers_mlp = 3\n",
        "hidden_dim = 300\n",
        "\n",
        "model = ResidualGatedGCNModel(num_nodes, num_layers_graph, num_layers_mlp, hidden_dim)\n",
        "model = model.to(device)\n",
        "\n",
        "# create optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=5e-4)\n",
        "\n",
        "# Main\n",
        "for epoch in range(300):\n",
        "\n",
        "    loss = train(model, features, labels, train_mask)\n",
        "    val_acc = evaluate(model, features, labels, valid_mask)\n",
        "\n",
        "    # if epoch % 10 == 9:\n",
        "    print(\"Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f}\".format(epoch+1, loss, val_acc))\n",
        "\n",
        "# Testing\n",
        "test_acc = evaluate(model, features, labels, test_mask)\n",
        "print(\"Test Accuracy {:.4f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-aea033f30512>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-9e2049b0fc9c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, features, labels, mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mlogp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
          ]
        }
      ]
    }
  ]
}
